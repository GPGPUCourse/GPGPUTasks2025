1) При работе на гпу мы можем выгрузить входной массив в шаред и легко получать доступ ко соседним элементам входных данных, которые будут
(не всегда, но в большинстве случаев кроме, когда переходим на сосений sm) рядом, это быстро и каждый поток может независимо это делать. В случае y2[n] = y2[n - 2] + y2[n - 1] + x[n] нужно 
ждать результат вычисления предыдущих двух потоков (n-1, n-2) и это вообще не подходит под идею параллелизма.

2) Если я правильно понял get_local_id(0)=threadIdx.x, get_local_size(0)=blockDim.x, get_local_id(1)=threadIdx.y и 
int idx = threadIdx.y + threadIdx.x * 32, то есть потоки по х всегда  кратны 32 и всегда уходят полностью в одно условие if/else без code divergence.

3) 
а) data[threadIdx.x + 32 * threadIdx.y] = 1.0f; и 1f (4 байта)  * 32 элемента = 128 байта => выровнено в одну кеш-линию ровно под 1 варп для 32 подряд идущих значений х (потоков) и доступ coalesced.
б) data[threadIdx.y + 32 * threadIdx.x] => данные расположены в памяти строками по х, следовательно в данном случае доступ к элеменетам по х будет через 32 элемента в памяти (включитьельно)
и это не коалест и будем использовать 32 кэш линии на варп. ну или лилипуты из одной машины будут долго ждать свой заказ из 32 разных блюд, повару долго готовить разные блюда из меню.
в) data[1 + threadIdx.x + 32 * threadIdx.y] => данные сдвинуты за предел одной кэш линии на 1 элемент и всяегда будут задействоать соседний кэш. и получется не коалест с перекрытием 
2 кэшлинии. Всего 2 разных блюда и повар их быстро приготовит, но не так, как в первом случае
